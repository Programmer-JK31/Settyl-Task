{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CWW-BfNznk_5"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf     #for tensorflow\n",
        "from sklearn.model_selection import train_test_split    #to create test & train set\n",
        "from sklearn.preprocessing import LabelEncoder    #to preprocess data in terms of numpy arrays\n",
        "from sklearn.feature_extraction.text import CountVectorizer #to preprocess data in terms of numpy arrays\n",
        "from sklearn.pipeline import Pipeline #easy to code than tensorflow\n",
        "from sklearn.ensemble import RandomForestClassifier #for random forest model\n",
        "from sklearn.linear_model import LogisticRegression #for logistic regression model\n",
        "from sklearn.metrics import accuracy_score #to test the accuracy of model\n",
        "from sklearn.naive_bayes import GaussianNB #to test GaussianNB model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess dataset\n",
        "# def preprocess_dataset(datasetset):\n",
        "#     # external_status_mapping = {}\n",
        "#     # internal_status_mapping = {}\n",
        "#     # for entry in datasetset[\"externalStatus\"]:\n",
        "#     #     external_status = entry\n",
        "#     #     if external_status not in external_status_mapping:\n",
        "#     #         external_status_mapping[external_status] = len(external_status_mapping)\n",
        "\n",
        "#     # for entry in datasetset[\"internalStatus\"]:\n",
        "#     #     internal_status = entry\n",
        "#     #     if internal_status not in internal_status_mapping:\n",
        "#     #         internal_status_mapping[internal_status] = len(internal_status_mapping)\n",
        "#     # X_train = np.array([external_status_mapping[entry] for entry in datasetset[\"externalStatus\"]])\n",
        "#     # y_train = np.array([internal_status_mapping[entry] for entry in datasetset[\"internalStatus\"]])\n",
        "#     return X_train, y_train"
      ],
      "metadata": {
        "id": "6PVpBkAXomwW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Loading data from dataset.json file ####\n",
        "dataset = pd.read_json('dataset.json')\n",
        "X_train = dataset['externalStatus']\n",
        "Y_train = dataset['internalStatus']"
      ],
      "metadata": {
        "id": "kqk0sUezop1H"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "ngsQSs3n6KQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1>**Using Tensorflow Keras**"
      ],
      "metadata": {
        "id": "dxHcH2qytvxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Encoding categorical variables ####\n",
        "label_encoder_x = LabelEncoder()\n",
        "dataset[\"externalStatus\"] = label_encoder_x.fit_transform(dataset[\"externalStatus\"])\n",
        "num_classes = len(label_encoder_x.classes_)\n",
        "\n",
        "label_encoder_y = LabelEncoder()\n",
        "dataset[\"internalStatus\"] = label_encoder_y.fit_transform(dataset[\"internalStatus\"])\n",
        "\n",
        "#### Spliting data into features (x) and target (y) ####\n",
        "x = dataset[\"externalStatus\"]\n",
        "y = dataset[\"internalStatus\"]\n",
        "\n",
        "#### Spliting data into train and validation sets ####\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#### Defining the model architecture ####\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(num_classes, activation='sigmoid', input_shape=(1,))\n",
        "])\n",
        "\n",
        "#### Compiling the model ####\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#### Training the model ####\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=10)\n",
        "\n",
        "#### Evaluating the model ####\n",
        "loss, accuracy = model.evaluate(x_val, y_val)\n",
        "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
        "\n",
        "#print(\"Model development completed.\")\n",
        "\n",
        "# softmax activavtion accuracy = 0.4244897961616516\n",
        "# ReLU (Rectified Linear Unit) activation accuracy = 0.2489795982837677\n",
        "# Sigmoid activation accuracy = 0.4285714328289032"
      ],
      "metadata": {
        "id": "4x_GdGjW7zmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### to get the graph to analyse relationship ####\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVJZR09ApecD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Preprocessing Input ####\n",
        "def preprocess_input(input_data, label_encoder):\n",
        "    # Encoding input to use in our model\n",
        "    encoded_input = label_encoder.fit_transform([input_data])\n",
        "    return np.array(encoded_input)"
      ],
      "metadata": {
        "id": "YaV__CMmxVSr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Predicting the output of User Input ####\n",
        "def modelPrediction(User_Input):\n",
        "  encoded_input = preprocess_input(User_Input, label_encoder_x)\n",
        "\n",
        "  # Making predictions using the trained model\n",
        "  predictions = model.predict(encoded_input)\n",
        "\n",
        "  # Decoding the predictions to get the class labels\n",
        "  predicted_class_index = np.argmax(predictions)\n",
        "  dataset = pd.read_json('dataset.json')\n",
        "  predicted_class_label = dataset[\"internalStatus\"][label_encoder_y.fit_transform([predicted_class_index])]\n",
        "\n",
        "  print(f\"Predicted internal status for external status '{User_Input}': {predicted_class_label}\")"
      ],
      "metadata": {
        "id": "1jb__d4EAqYU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1> Using Gaussian NB Model</H1>"
      ],
      "metadata": {
        "id": "sIlEdXlxSoqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Loading data from dataset.json file ####\n",
        "dataset = pd.read_json('dataset.json')\n",
        "X = dataset[\"externalStatus\"]\n",
        "y = dataset[\"internalStatus\"]"
      ],
      "metadata": {
        "id": "_WAa3nwdS-Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Defining the mdoel Architecture ####\n",
        "model = GaussianNB()\n",
        "\n",
        "#### Encoding Data from dataset ####\n",
        "label_encoder_x = LabelEncoder()\n",
        "X = label_encoder_x.fit_transform(dataset[\"externalStatus\"]).reshape(-1, 1)\n",
        "num_classes = len(label_encoder_x.classes_)\n",
        "\n",
        "label_encoder_y = LabelEncoder()\n",
        "y = label_encoder_y.fit_transform(dataset[\"internalStatus\"]).reshape(-1, 1)\n",
        "\n",
        "#### Spliting data into train and validation sets ####\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#### Training the model ####\n",
        "model_f = model.fit(X_train,y_train)\n",
        "\n",
        "print(\"model accuracy : \", model.score(X_test,y_test))\n",
        "\n",
        "# model accuracy :  0.7795918367346939"
      ],
      "metadata": {
        "id": "8aPXVtIqTIx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mb1_XdHtZJuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1>**Using Random Forest**\n"
      ],
      "metadata": {
        "id": "3Plv10V4NjZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_json('dataset.json')\n",
        "X = dataset[\"externalStatus\"]\n",
        "y = dataset[\"internalStatus\"]\n",
        "# Spliting data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining a pipeline with CountVectorizer and RandomForestClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Training the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluating model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Accuracy = 0.9959183673469387"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ByrI9SXNhkc",
        "outputId": "d892f0d8-1443-47c3-a4f1-7013c28cde5f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9959183673469387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>**Using Logistic Regression**"
      ],
      "metadata": {
        "id": "J3gQY5t9tXSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_json('dataset.json')\n",
        "X = dataset[\"externalStatus\"]\n",
        "y = dataset[\"internalStatus\"]\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining a pipeline with CountVectorizer and LogisticRegression\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Training the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluating model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Accuracy = 0.9959183673469387"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "topRQQCB6Knx",
        "outputId": "76e674b9-4cac-40bd-93c2-e70941b01fec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9959183673469387\n"
          ]
        }
      ]
    }
  ]
}