# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EAP-5WCivIRwksklXKLFjCSOYNBYZf1k
"""

#importing dependencies

import pandas as pd
import numpy as np
import tensorflow as tf     #for tensorflow
from sklearn.model_selection import train_test_split    #to create test & train set
from sklearn.preprocessing import LabelEncoder    #to preprocess data in terms of numpy arrays
from sklearn.feature_extraction.text import CountVectorizer #to preprocess data in terms of numpy arrays
from sklearn.pipeline import Pipeline #easy to code than tensorflow
from sklearn.ensemble import RandomForestClassifier #for random forest model
from sklearn.linear_model import LogisticRegression #for logistic regression model
from sklearn.metrics import accuracy_score #to test the accuracy of model
from sklearn.naive_bayes import GaussianNB #to test GaussianNB model

# Preprocess dataset
# def preprocess_dataset(datasetset):
#     # external_status_mapping = {}
#     # internal_status_mapping = {}
#     # for entry in datasetset["externalStatus"]:
#     #     external_status = entry
#     #     if external_status not in external_status_mapping:
#     #         external_status_mapping[external_status] = len(external_status_mapping)

#     # for entry in datasetset["internalStatus"]:
#     #     internal_status = entry
#     #     if internal_status not in internal_status_mapping:
#     #         internal_status_mapping[internal_status] = len(internal_status_mapping)
#     # X_train = np.array([external_status_mapping[entry] for entry in datasetset["externalStatus"]])
#     # y_train = np.array([internal_status_mapping[entry] for entry in datasetset["internalStatus"]])
#     return X_train, y_train

#### Loading data from dataset.json file ####
dataset = pd.read_json('dataset.json')
X_train = dataset['externalStatus']
Y_train = dataset['internalStatus']

dataset.head()

"""<H1>**Using Tensorflow Keras**"""

#### Encoding categorical variables ####
label_encoder_x = LabelEncoder()
dataset["externalStatus"] = label_encoder_x.fit_transform(dataset["externalStatus"])
num_classes = len(label_encoder_x.classes_)

label_encoder_y = LabelEncoder()
dataset["internalStatus"] = label_encoder_y.fit_transform(dataset["internalStatus"])

#### Spliting data into features (x) and target (y) ####
x = dataset["externalStatus"]
y = dataset["internalStatus"]

#### Spliting data into train and validation sets ####
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

#### Defining the model architecture ####
model = tf.keras.Sequential([
    tf.keras.layers.Dense(num_classes, activation='sigmoid', input_shape=(1,))
])

#### Compiling the model ####
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#### Training the model ####
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=10)

#### Evaluating the model ####
loss, accuracy = model.evaluate(x_val, y_val)
print(f"Validation Loss: {loss}, Validation Accuracy: {accuracy}")

#print("Model development completed.")

# softmax activavtion accuracy = 0.4244897961616516
# ReLU (Rectified Linear Unit) activation accuracy = 0.2489795982837677
# Sigmoid activation accuracy = 0.4285714328289032

#### to get the graph to analyse relationship ####
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#### Preprocessing Input ####
def preprocess_input(input_data, label_encoder):
    # Encoding input to use in our model
    encoded_input = label_encoder.fit_transform([input_data])
    return np.array(encoded_input)

#### Predicting the output of User Input ####
def modelPrediction(User_Input):
  encoded_input = preprocess_input(User_Input, label_encoder_x)

  # Making predictions using the trained model
  predictions = model.predict(encoded_input)

  # Decoding the predictions to get the class labels
  predicted_class_index = np.argmax(predictions)
  dataset = pd.read_json('dataset.json')
  predicted_class_label = dataset["internalStatus"][label_encoder_y.fit_transform([predicted_class_index])]

  print(f"Predicted internal status for external status '{User_Input}': {predicted_class_label}")

"""<H1> Using Gaussian NB Model</H1>"""

#### Loading data from dataset.json file ####
dataset = pd.read_json('dataset.json')
X = dataset["externalStatus"]
y = dataset["internalStatus"]

#### Defining the mdoel Architecture ####
model = GaussianNB()

#### Encoding Data from dataset ####
label_encoder_x = LabelEncoder()
X = label_encoder_x.fit_transform(dataset["externalStatus"]).reshape(-1, 1)
num_classes = len(label_encoder_x.classes_)

label_encoder_y = LabelEncoder()
y = label_encoder_y.fit_transform(dataset["internalStatus"]).reshape(-1, 1)

#### Spliting data into train and validation sets ####
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#### Training the model ####
model_f = model.fit(X_train,y_train)

print("model accuracy : ", model.score(X_test,y_test))

# model accuracy :  0.7795918367346939



"""<H1>**Using Random Forest**

"""

dataset = pd.read_json('dataset.json')
X = dataset["externalStatus"]
y = dataset["internalStatus"]
# Spliting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Defining a pipeline with CountVectorizer and RandomForestClassifier
pipeline = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('classifier', RandomForestClassifier())
])

# Training the model
pipeline.fit(X_train, y_train)

# Making predictions on test data
y_pred = pipeline.predict(X_test)

# Evaluating model performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Accuracy = 0.9959183673469387

"""<h1>**Using Logistic Regression**"""

dataset = pd.read_json('dataset.json')
X = dataset["externalStatus"]
y = dataset["internalStatus"]
# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Defining a pipeline with CountVectorizer and LogisticRegression
pipeline = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('classifier', LogisticRegression())
])

# Training the model
pipeline.fit(X_train, y_train)

# Making predictions on test data
y_pred = pipeline.predict(X_test)

# Evaluating model performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Accuracy = 0.9959183673469387

def predictFromInput(External_Status : str):
    return pipeline.predict([External_Status])[0]
